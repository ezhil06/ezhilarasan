# Exposing the truth with advanced fake news detection powered by natural language processing
import pandas as pd
import re
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
import joblib  # For model persistence
from google.colab import files

# Ensure reproducibility
np.random.seed(42)

# Download necessary NLTK resources
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

# Streamlit UI
!pip install streamlit
import streamlit as st
st.set_page_config(page_title="Fake News Detector", layout="centered")
st.title("Fake News Detection Web App")
st.markdown("Upload a dataset, train a model, and test with your own news text.")

# Load and prepare data
uploaded = files.upload()
uploaded = files.upload()

# Load both datasets
fake_df = pd.read_csv("Fake.csv")
true_df = pd.read_csv("True.csv")

# Assign labels
fake_df["label"] = 0  # Fake news
true_df["label"] = 1  # Real news

# Combine datasets
df = pd.concat([fake_df, true_df], ignore_index=True)

if df is not None:
  # Drop missing values
    df = df.dropna()
    print("\nDataFrame Head:")
    print(df.head())
    print("\nDataFrame Info:")
    print(df.info())
    print("\nClass Distribution:\n", df['label'].value_counts())

    # Initialize text processing tools outside of function
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words('english'))

    def clean_text(text):
        text = re.sub('[^a-zA-Z]', ' ', text)
        text = text.lower()
        words = text.split()
        words = [stemmer.stem(word) for word in words if word not in stop_words]
        return ' '.join(words)

    df['text'] = df['text'].apply(clean_text)
    print("First five cleaned entries:")
    print(df['text'].head())

    # Encode labels if not already numeric
    if df['label'].dtype != 'int64':
        le = LabelEncoder()
        df['label'] = le.fit_transform(df['label'])

    print("\nClass Distribution:\n", df['label'].value_counts())

    X = df['text']
    y = df['label']

    # TF-IDF vectorization
    tfidf_vectorizer = TfidfVectorizer(max_df=0.7)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    tfidf_train = tfidf_vectorizer.fit_transform(X_train)
    tfidf_test = tfidf_vectorizer.transform(X_test)

    print("\nFeature Extraction Complete.")

    # Train Logistic Regression with balanced class weights
    model = LogisticRegression(class_weight='balanced')
    model.fit(tfidf_train, y_train)
    print("\nModel Training Complete.")

    y_pred = model.predict(tfidf_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nAccuracy: {accuracy:.2f}")
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    # Visualization
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title("Confusion Matrix")
    plt.show()

    # Save model and vectorizer
    joblib.dump(model, "fake_news_model.joblib")
    joblib.dump(tfidf_vectorizer, "tfidf_vectorizer.joblib")
    print("Model and vectorizer saved.")

    # Example prediction
    example_text = ["This is a breaking news story that is completely fabricated and untrue."]
    cleaned_example = [clean_text(text) for text in example_text]
    tfidf_example = tfidf_vectorizer.transform(cleaned_example)
    prediction = model.predict(tfidf_example)[0]
    print(f"Example Prediction: {prediction} (1=Real, 0=Fake)")

    # Show classification report
    st.subheader("Classification Report")
    report = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    st.dataframe(report_df.style.highlight_max(axis=0))

    # Prediction interface
    st.subheader("Test the Model")
    user_input = st.text_area("Enter news text to classify")

    if st.button("Predict"):
        cleaned_input = clean_text(user_input)
        vectorized_input = tfidf_vectorizer.transform([cleaned_input])
        prediction = model.predict(vectorized_input)[0]
        label = "Real" if prediction == 1 else "Fake"
        st.success(f"Prediction: **{label}**")

    # Automated EDA Profiling (ydata-profiling, Sweetviz, D-Tale)
    # Install and generate profiling reports (for Colab environments)
    !pip install ydata-profiling sweetviz dtale
    from ydata_profiling import ProfileReport
    import sweetviz as sv
    import dtale

    profile = ProfileReport(df, explorative=True)
    profile.to_file("Fake_Detection_Report.html")

    sweet_report = sv.analyze(df)
    sweet_report.show_html("Fake_Detection_sweetviz.html")

    dtale.show(df)

    # Hyperparameter Tuning
    param_grid = {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}
    grid = GridSearchCV(LogisticRegression(), param_grid, cv=2)
    grid.fit(tfidf_train, y_train)
    print("Best Parameters:", grid.best_params_)
    model = grid.best_estimator_

    # Save modified dataset
    df.to_csv("Fake_Modified.csv", index=False)
    files.download("Fake_Modified.csv")

else:
    print("Please correct the data loading step before continuing.")
